---
title: "MAT02035 - Modelos para dados correlacionados"
subtitle: "Estimação e inferência estatística"
author: |
  | Rodrigo Citton P. dos Reis
  | `citton.padilha@ufrgs.br`
institute: |
  | \textsc{Universidade Federal do Rio Grande do Sul}
  | \textsc{Instituto de Matemática e Estatística}
  | \textsc{Departamento de Estatística}
date: |
  | Porto Alegre, 2019
---

# Introdução

## Introdução

- Até agora, nossa discussão de modelos para dados longitudinais tem sido muito geral, sem menção de métodos para estimar os coeficientes de regressão ou a covariância entre as medidas repetidas.
- __Relembrando:__ o modelo de regressão linear geral para o vetor de resposta média

$$
\E(Y_i|X_i) = X_i\beta.
$$

- Assumimos que o vetor de respostas, $Y_i$, tem uma distribuição condicional que é __normal multivariada__ com matriz de covariância

$$
\Cov(Y_i|X_i) = \Sigma_i = \Sigma_i(\theta).
$$

- Note que $\theta$ é um vetor $q \times 1$ de __parâmetros de covariância__.

## Introdução

- Dados balanceados ($n_i = n$), em que covariância "não-estruturada" é assumida, temos $n$ variâncias e $\frac{n(n - 1)}{2}$ covariâncias como elementos do vetor $\theta$.
- Se a covariância é assumida ter um padrão de "simetria composta", então $q = 2$ e $\theta$ tem dois elementos.
- Nesta aula, consideramos uma estrutura para estimativa dos parâmetros desconhecidos, $\beta$ e $\theta$ (ou equivalentemente, $\Sigma_i$).
    
# Estimação: máxima verossimilhança

## Estimação: máxima verossimilhança

- Dado que foram feitas suposições completas sobre a distribuição do vetor de respostas, $Y_i$, uma abordagem muito geral de estimativa é o __método da máxima verossimilhança__ (MV).
    + A ideia fundamental por trás da estimativa de MV é realmente bastante simples e é transmitida por seu nome: use como estimativas de $\beta$ e $\theta$ os valores que são mais prováveis (ou mais "verossímeis") para os dados que foram realmente observados.
    + As estimativas de verossimilhança máxima de $\beta$ e $\theta$ são aqueles valores de $\beta$ e $\theta$ que maximizam a probabilidade conjunta das variáveis resposta __avaliadas em seus valores observados__.

## Estimação: máxima verossimilhança

- A probabilidade das variáveis de resposta avaliadas no conjunto fixo de valores observados e consideradas como funções de $\beta$ e $\Sigma_i(\theta)$ é conhecida como __função de verossimilhança__.
    + Assim, a estimativa de $\beta$ e $\theta$ prossegue __maximizando__ a função de verossimilhança.
- Os valores de $\beta$ e $\theta$ que maximizam a função de verossimilhança são chamados de __estimativas de máxima verossimilhança__ de $\beta$ e $\theta$, e geralmente são indicados $\hat{\beta}$ e $\hat{\theta}$ (ou $\widehat{\Sigma}_i$, $\Sigma_i(\hat{\theta})$ ).

## Observações independentes

- Suponha que os dados surjam de uma série de estudos transversais que são repetidos em $n$ ocasiões diferentes.
- Em cada ocasião, os dados são obtidos em uma amostra de $N$ indivíduos.
    + Aqui é razoável supor que as observações sejam __independentes__ umas das outras, uma vez que cada indivíduo é medido em apenas uma ocasião.
- Além disso, para facilitar a exposição, assumimos que a variância é constante, digamos a $\sigma^2$.
- A resposta média está relacionada às covariáveis através do seguinte modelo de regressão linear:

$$
\E(Y_{ij}|X_{ij}) = X_{ij}'\beta.
$$

## Observações independentes

- Para obter estimativas de máxima verossimilhança de $\beta$, devemos encontrar os valores dos parâmetros de regressão que maximizem a função de densidade de probabilidade normal conjunta de todas as observações, avaliados nos valores observados da resposta e considerados como uma função de $\beta$ (e $\sigma^2$).
- __Lembrando__ que a função de densidade de probabilidade normal (ou gaussiana) univariada para $Y_{ij}$, dado $X_{ij}$, pode ser expressa como

$$
f(y_{ij}) = (2\pi\sigma^2)^{-1/2}\exp\left\{-\frac{1}{2}(y_{ij} - \mu_{ij})^2/\sigma^2\right\},\ -\infty < y_{ij} < \infty.
$$

## Observações independentes

- Quando todas as respostas são independentes umas das outras, a função de verossimilhança é simplesmente o produto das funções individuais de densidade de probabilidade normal univariada para $Y_{ij}$ dado $X_{ij}$,

$$
\prod_{i=1}^N{\prod_{j=1}^n{f(y_{ij})}}.
$$

## Observações independentes

- É mais comum trabalhar com a função __log-verossimilhança__, que envolverá somas, em vez de produtos, das funções individuais de densidade de probabilidade normal univariada para $Y_{ij}$.
    + Observe que maximizar a verossimilhança é equivalente a maximizar o logaritmo da verossimilhança; o último é indicado por $l$.
- Portanto, o objetivo é maximizar

$$
l = \log\left\{\prod_{i=1}^N{\prod_{j=1}^n{f(y_{ij})}}\right\} = -\frac{K}{2} \log(2\pi\sigma^2) - \frac{1}{2}\sum_{i=1}^N{\sum_{j=1}^n{(y_{ij} - X_{ij}'\beta)^2/\sigma^2}},
$$

avaliado nos valores numéricos observados dos dados, em relação aos parâmetros de regressão, $\beta$.
- Aqui $K = n \times N$, o __número total de observações__.

## Observações independentes

- Observe que $\beta$ não aparece no primeiro termo da log-verossimilhança.
    + Como resultado, esse termo pode ser ignorado ao maximizar a log-verossimilhança em relação a $\beta$.
- Além disso, como o segundo termo tem um sinal negativo, maximizar a log-verossimilhança em relação a $\beta$ é equivalente a minimizar a seguinte função:

$$
\sum_{i=1}^N{\sum_{j=1}^n{(y_{ij} - X_{ij}'\beta)^2}}.
$$

## Observações independentes

- Maximizar ou minimizar uma função é um problema matemático comum que pode ser resolvido usando o cálculo.
- Especificamente, a estimativa de máxima verossimilhança de $\beta$ pode ser obtida igualando a derivada da log-verossimilhança, frequentemente chamada de __função escore__, a zero e encontrando a solução para a equação resultante.
- No entanto, no exemplo considerado aqui, não há necessidade real de recorrer ao cálculo.
- A obtenção da estimativa de máxima verossimilhança de $\beta$ é equivalente a encontrar a estimativa de __mínimos quadrados ordinários__ (MQO) de $\beta$, ou seja, o valor de $\beta$ que minimiza a soma dos quadrados dos resíduos.

## Observações independentes

- Usando a notação vetorial, a solução dos mínimos quadrados pode ser escrita como

$$
\hat{\beta} = \left\{\sum_{i=1}^N{\sum_{j=1}^n{(X_{ij}X_{ij}')}}\right\}^{-1}\sum_{i=1}^N{\sum_{j=1}^n{(X_{ij}y_{ij})}}.
$$

- Essa estimativa de mínimos quadrados é o valor produzido por qualquer _software_ estatístico padrão para regressão linear (por exemplo, `PROC GLM` ou `PROC REG` no `SAS`, a função `lm` no `R` e  o comando `regress` no `Stata`).
    + __Exercício:__ considere o modelo dos dados de nível de chumbo no sangue; compare as estimativas através de uma implementação sua de $\hat{\beta}$ com a função `lm`.
- Além disso, note que até agora apenas focamos na estimativa de $\beta$, ignorando a estimativa de $\sigma^2$; a seguir, também consideramos a estimativa da matriz de covariância.

## Observações correlacionadas

- Quando há $n_i$ medidas repetidas no mesmo indivíduo, __não se pode assumir que essas medidas repetidas são independentes__.
    + Como resultado, precisamos considerar a função de densidade de probabilidade conjunta para o __vetor de medidas repetidas__.
- Observe, no entanto, que os vetores de medidas repetidas são assumidos como __independentes__ uns dos outros.
    + Assim, a função log-verossimilhança, $l$, pode ser expressa como uma __soma das funções multivariadas individuais__ da densidade de probabilidade normal para $Y_i$ dado $X_i$.

## Observações correlacionadas

<!-- - Para encontrar a estimativa de máxima verossimilhança de $\beta$ na configuração de medidas repetidas,  -->
- Primeiro assumimos que $\Sigma_i$ (ou $\theta$) é conhecido (e, portanto, não precisa ser estimado); depois, relaxaremos essa suposição muito irrealista.
<!-- - Para obter a estimativa de máxima verossimilhança de $\beta$, precisamos encontrar o valor de $\beta$ que maximize a função de log-verossimilhança. -->
- Dado que $Y_i = (Y_{i1}, Y_{i2}, \ldots, Y_{in_i})'$ é assumido como tendo uma distribuição condicional que é normal multivariada, devemos maximizar a seguinte função de log-verossimilhança:
    
$$
l =  -\frac{K}{2} \log(2\pi) - \frac{1}{2}\sum_{i=1}^N{\log|\Sigma_i|} - \frac{1}{2}\left\{\sum_{i=1}^N{(y_i - X_i\beta)'\Sigma_i^{-1}(y_i - X_i\beta)}\right\},
$$

- $K = \sum_{i=1}^N{n_i}$ é o __número total de observações__.

## Observações correlacionadas

- Note que $\beta$ não aparece nos dois primeiros termos na log-verossimilhança.
    + Esses dois termos podem ser ignorados ao maximizar a log-verossimilhança em relação a $\beta$.
- Além disso, como o terceiro termo tem um sinal negativo, maximizar a log-verossimilhança em relação a $\beta$ é equivalente a minimizar

$$
\sum_{i=1}^N{(y_i - X_i\beta)'\Sigma_i^{-1}(y_i - X_i\beta)}.
$$

## Observações correlacionadas

- O estimador de $\beta$ que minimiza essa expressão é conhecido como estimador de __mínimos quadrados generalizados__ (MQG) de $\beta$ e pode ser expresso como

$$
\hat{\beta} = \left\{\sum_{i=1}^N{(X_i'\Sigma_i^{-1}X_i)}\right\}^{-1}\sum_{i=1}^N{(X_i'\Sigma_i^{-1}y_i)}.
$$

- Veja a função `gls` do pacote `nlme` do `R`.

## Propriedades do estimador MQG ($\Sigma_i$ conhecida)

- A primeira propriedade muito notável é que, para __qualquer escolha__ de $\Sigma_i$, a estimativa MQG de $\beta$ é __não viesada__. Ou seja,

$$
\E[\hat{\beta}] = \beta.
$$

- Além disso, __em amostras grandes__ (ou assintoticamente), pode se mostrar que a distribuição amostral de $\hat{\beta}$ é uma distribuição normal multivariada com média $\beta$ e covariância

$$
\Cov(\hat{\beta}) = \left\{\sum_{i=1}^N{(X_i'\Sigma_i^{-1}X_i)}\right\}^{-1}.
$$

## Propriedades do estimador MQG ($\Sigma_i$ conhecida)

- Isso é verdade exatamente quando $Y_i$ tem uma distribuição condicional que é multivariada normal e verdadeira em __amostras grandes__, mesmo quando a distribuição condicional de $Y_i$ não é normal multivariada.
    + Por "grandes amostras", entendemos que o tamanho da amostra, $N$, aumenta quando o número de medidas repetidas e parâmetros do modelo permanece fixo.
- Observe também que se $\Sigma_i$ for assumido como uma matriz diagonal, com variância constante $\sigma^2$ ao longo da diagonal, o estimador MQG reduz para o estimador de mínimos quadrados ordinários considerado mais cedo.
    + __Exercício:__ demonstre este resultado.
- Finalmente, embora o estimador MQG de $\beta$ seja não viesado para qualquer escolha de $\Sigma_i$, pode ser mostrado que o estimador MQG mais eficiente de $\beta$ (ou seja, o estimador com menor variância ou maior precisão) é aquele que usa o valor verdadeiro de $\Sigma_i$.
    + __Pergunta:__ o que isto quer dizer?

## MQG ($\Sigma_i$ desconhecida)

- Vamos abordar o caso que geralmente ocorre na prática: não conhecemos $\Sigma_i$ (ou $\theta$).
    + Neste caso precisamos estimar $\Sigma_i$ ($\theta$) a partir dos dados disponíveis.
- A estimativa da máxima verossimilhança de $\theta$ prossegue da mesma maneira que a estimativa de $\beta$, maximizando a log-verossimilhança em relação a $\theta$.
<!-- - Como mencionado anteriormente, o problema de maximizar uma função é um problema matemático comum que pode ser resolvido usando o cálculo. -->
-  Especificamente, a estimativa de máxima verossimilhança de $\theta$ pode ser obtida igualando a derivada da log-verossimilhança em relação a $\theta$ (função escore) a zero e encontrando a solução para a equação resultante.
- Entretanto, em geral, essa __equação é não linear__ e não é possível escrever expressões simples e de forma fechada para o estimador de MV de $\theta$.
    + A estimativa de MV deve ser encontrada resolvendo-se essa equação usando uma técnica __iterativa__.

## MQG ($\Sigma_i$ desconhecida)

- Felizmente, algoritmos de computador foram desenvolvidos para encontrar a solução.
- Uma vez obtida a estimativa de MV de $\theta$, simplesmente substituímos a estimativa de $\Sigma_i(\theta)$, digamos $\widehat{\Sigma}_i = \Sigma_i(\hat{\theta})$, no estimador de mínimos quadrados generalizados de $\beta$ para obter a estimativa de MV de $\beta$:


$$
\hat{\beta} = \left\{\sum_{i=1}^N{(X_i'\widehat{\Sigma}_i^{-1}X_i)}\right\}^{-1}\sum_{i=1}^N{(X_i'\widehat{\Sigma}_i^{-1}y_i)}.
$$

## Propriedades do estimador MQG ($\Sigma_i$ conhecida)

- Curiosamente, __em amostras grandes__ (ou assintoticamente), o estimador resultante de $\beta$ que substitui a estimativa de MV de $\Sigma_i$ tem todas as __mesmas propriedades__ de quando $\Sigma_i$ é realmente conhecido.

- Assim, em termos de propriedades da distribuição amostral de $\hat{\beta}$, não há penalidade por realmente ter que estimar $\Sigma_i$ a partir dos dados longitudinais em questão.
- No entanto, por mais reconfortante que esse resultado possa parecer, deve-se ter em mente que esta é uma propriedade de grande amostra (ou seja, quando $N$ se aproxima do infinito) de $\hat{\beta}$.
    + Com tamanhos de amostra da magnitude frequentemente encontrados em muitas áreas de aplicação, pode-se esperar que as propriedades da distribuição amostral de $\hat{\beta}$ sejam adversamente influenciadas pela estimativa de um número muito grande de parâmetros de covariância.

# Questões de dados ausentes

## Questões de dados ausentes

Embora a maioria dos estudos longitudinais seja projetada para coletar dados de cada indivíduo da amostra a cada momento do acompanhamento, muitos estudos têm algumas observações ausentes.

Dados ausentes têm três implicações importantes para a análise longitudinal:

1. O conjunto de dados é necessariamente desbalanceado ao longo do tempo, pois nem todos os indivíduos têm o mesmo número de medições repetidas em um conjunto comum de ocasiões.
    + Como resultado, os métodos de análise precisam ser capazes de lidar com os dados desequilibrados sem precisar descartar dados de indivíduos com dados ausentes.

## Questões de dados ausentes

2. Haverá perda de informações e redução na precisão com que mudanças na resposta média ao longo do tempo pode ser estimado.
    + Essa redução na precisão está diretamente relacionada à quantidade de dados ausentes e também será influenciada em certa medida pela maneira como a análise lida com os dados ausentes.
    + Por exemplo, usar apenas os casos completos (ou seja, aqueles indivíduos sem dados ausentes) geralmente será o método menos eficiente.
3. A validade de qualquer método de análise exigirá que certas suposições sobre os motivos de qualquer falta, geralmente chamadas de __mecanismo de perda de dados__, sejam sustentáveis.
    + Consequentemente, quando faltam dados, devemos considerar cuidadosamente os motivos da falta.

## Mecanismo de perda de dados

- O mecanismo de perda de dados pode ser pensado como __um modelo__ que descreve a probabilidade de uma resposta ser observada ou ausente em qualquer ocasião.
- Fazemos uma distinção importante entre mecanismos de dados ausentes que são referidos como __ausentes completamente ao acaso__ (_missing completely at random_ - MCAR) e __ausentes ao acaso__ (_missing at random_ - MAR).
- A distinção entre esses dois mecanismos determina a adequação da estimativa de máxima verossimilhança sob o pressuposto de uma distribuição normal multivariada para as respostas e o MQG sem exigir suposições sobre o formato da distribuição.

## MCAR

- Diz-se que os dados são MCAR quando a probabilidade de perda de respostas não está relacionada aos valores específicos que, em princípio, deveriam ter sido obtidos (as respostas ausentes) ou ao conjunto de respostas observadas.
- Ou seja, dados longitudinais são MCAR quando a falta em $Y_i$ é simplesmente o resultado de um mecanismo de chance que não depende de componentes observados ou não observados de $Y_i$.
- A característica essencial do MCAR é que os dados observados podem ser considerados uma amostra aleatória dos dados completos.
    + Como resultado, os momentos (por exemplo, médias, variâncias e covariâncias) e, de fato, a distribuição dos dados observados não diferem dos momentos correspondentes ou da distribuição dos dados completos.
    
## MCAR

<!-- - Um mecanismo MCAR tem consequências importantes para a análise de dados longitudinais. -->
- Qualquer método de análise que produza inferências válidas na ausência de dados ausentes também produzirá inferências válidas quando os dados ausentes forem MCAR e a análise for baseada em todos os dados disponíveis, ou mesmo quando estiver restrito aos "completadores" (ou seja, aqueles sem dados ausentes).
- Dado que estimativas válidas das médias, variâncias e covariâncias podem ser obtidas, o MQG fornece estimativas válidas de $\beta$ sem exigir nenhuma premissa de distribuição para $Y_i$.
- O estimador MQG de $\beta$ é válido, desde que o modelo para a resposta média tenha sido especificado corretamente; não requer nenhuma suposição sobre a distribuição conjunta das respostas longitudinais.

## MCAR

- O estimador de MV de $\beta$, pressupondo que as respostas tenham uma distribuição normal multivariada, também é o estimador MQG (com a estimativa MV de $\Sigma_i(\theta)$, por exemplo, $\widehat{\Sigma}_i = \Sigma_i(\hat{\theta})$, substituída).
- Assim, nessa configuração, os estimadores MV e MQG têm exatamente as mesmas propriedades, independentemente da verdadeira distribuição de $Y_i$.

## MAR

- Ao contrário do MCAR, diz-se que os dados são MAR quando a probabilidade de perda de respostas depende do conjunto de respostas observadas, mas não está relacionada aos valores ausentes específicos que, em princípio, deveriam ter sido obtidos.
- Em outras palavras, se os indivíduos são estratificados com base em valores semelhantes para as respostas observadas, a falta é simplesmente o resultado de um mecanismo de chance que não depende dos valores das respostas não observadas.
- No entanto, como o mecanismo de falta agora depende das respostas observadas, a distribuição de $Y_i$ em cada um dos estratos distintos definidos pelos padrões de falta não é o mesmo que a distribuição de $Y_i$ na população alvo.
- Isso tem consequências importantes para a análise.

## MAR

- Uma é que uma análise restrita aos "completadores" não é válida.
    + Em outras palavras, os "completadores" são uma amostra tendenciosa da população-alvo.
- Além disso, a distribuição dos componentes observados de $Y_i$, em cada um dos estratos distintos definidos pelos padrões de falta, não coincide com a distribuição dos mesmos componentes de $Y_i$ na população alvo.
    + Portanto, as médias amostrais, variâncias e covariâncias com base nos "completadores" ou nos dados disponíveis são estimativas tendenciosas dos parâmetros correspondentes na população-alvo.

## MAR

- Como resultado, o MQG não fornece mais estimativas válidas de $\beta$ sem fazer suposições corretas sobre a distribuição conjunta das respostas longitudinais.
- Por outro lado, a estimativa de MV de $\beta$ é válida quando os dados são MAR, desde que a distribuição normal multivariada foi especificada corretamente.
    + Isso requer a especificação correta não apenas do modelo para a resposta média, mas também do modelo para a covariância entre as respostas.
- Em certo sentido, a estimativa de MV permite que os valores ausentes sejam validamente "previstos" ou "imputados" usando os dados observados e um modelo correto para a distribuição conjunta das respostas.

# Inferência estatística

## Inferência estatística

- Para construir __intervalos de confiança__ e __testes de hipóteses__ sobre $\beta$, podemos fazer uso direto das estimativas de MV $\hat{\beta}$ e da sua matriz de covariância estimada 

$$
\widehat{\Cov}(\hat{\beta}) = \left\{\sum_{i=1}^N{(X_i'\widehat{\Sigma}_i^{-1}X_i)}\right\}^{-1}.
$$

## Inferência estatística

- Para um único componente de $\beta$, digamos $\beta_k$, um método natural de construção de __limites de confiança__ de 95% é tomar o $\hat{\beta}_k$ mais ou menos $1,96$ vezes o erro padrão de $\hat{\beta}_k$.
    + Diferentes limites de confiança podem ser obtidos escolhendo os quantis apropriados da distribuição normal padrão.
- O erro padrão de $\hat{\beta}_k$ é simplesmente a raiz quadrada do elemento da diagonal principal de $\widehat{\Cov}(\hat{\beta})$ correspondente a $\hat{\beta}_k$,

$$
\sqrt{\widehat{\Var}(\hat{\beta}_k)}.
$$

## Inferência estatística

- De maneira similar, um __teste da hipótese__ nula, $H_0: \beta_k = 0$ versus $H_A: \beta_k \neq 0$, pode ser baseado na seguinte estatística de Wald:

$$
Z = \frac{\hat{\beta}_k}{\sqrt{\widehat{\Var}(\hat{\beta}_k)}},
$$

em que $\widehat{\Var}(\hat{\beta}_k)$ denota o elemento da diagonal principal de $\widehat{\Cov}(\hat{\beta})$ correspondente a $\hat{\beta}_k$.

- Esta estatística de teste pode ser comparada com uma distribuição normal padrão.

## Inferência estatística

- De maneira mais geral, pode ser de interesse construir intervalos de confiança e testes de hipóteses com respeito a certas __combinações lineares__ dos componentes de $\beta$.
- Seja $L$ um vetor ou uma matriz de pesos __conhecidos__ e suponha que é de interesse testar $H_0: L\beta = 0$.
- A combinação linear dos componentes de $\beta$, $L\beta$, representa um __contraste__ de interesse científico.
    + Exemplo: suponha que $\beta = (\beta_1, \beta_2, \beta_3)'$ e $L = (0, 0, 1)$, então $H_0:L\beta = 0$ é equivalente a $H_0: \beta_3 = 0$.
        - Agora, se considerarmos $L = (0, 1, -1)$, então $H_0:L\beta = 0$ é equivalente a $H_0: \beta_2 - \beta_3 = 0$ ou $H_0: \beta_2 = \beta_3$.

## Inferência estatística

- Uma estimativa natural de $L\beta$ é $L\hat{\beta}$, e pode ser mostrado que $L\hat{\beta}$ tem distribuição normal multivariada com média $L\beta$ e matriz de covariância $L\Cov(\hat{\beta})L'$.

- Nos dois exemplos anteriores, $L$ é um único vetor linha $1 \times 3$, $L = (0, 0, 1)$ ou $L = (0, 1, -1)$.
- Se $L$ é um único vetor linha, então $L\Cov(\hat{\beta})L'$ é um único valor (escalar) e a sua raiz quadrada fornece uma estimativa do erro padrão para $L\hat{\beta}$.
- Assim um intervalo de confiança de aproximadamente 95% para $L\beta$ é dado por

$$
L\hat{\beta} \pm 1,96\sqrt{L\widehat{\Cov}(\hat{\beta})L'}.
$$

## Inferência estatística

- De forma similar, para testar $H_0: L\beta = 0$ versus $H_A: L\beta \neq 0$, podemos usar a estatística de Wald

$$
Z = \frac{L\hat{\beta}}{\sqrt{L\widehat{\Cov}(\hat{\beta})L'}},
$$

e comparar esta estatística de teste com a distribuição normal padrão.

- Um teste idêntico para $H_0: L\beta = 0$ versus $H_A: L\beta \neq 0$ usa a estatística

$$
W^2 = (L\hat{\beta})\{L\widehat{\Cov}(\hat{\beta})L'\}^{-1}(L\hat{\beta}),
$$

e comparar $W^2$ com a distribuição $\chi^2$ com $1$ grau de liberdade (__por que?__).

## Inferência estatística

- Esta última observação nos ajuda a motivar como o teste de Wald prontamente generaliza quando $L$ tem mais que uma linha, permitindo o teste simultâneo de uma hipótese multivariada.
- Por exemplo, suponha que $\beta = (\beta_1, \beta_2, \beta_3)'$ e é de interesse testar a igualdade dos três parâmetros de regressão.
    + A hipótese nula pode ser expressa como $H_0: \beta_1 = \beta_2 = \beta_3$. Fazendo

$$
L = \left(\begin{array}{ccc}
1 & -1 & 0 \\
1 & 0 & -1
\end{array}\right).
$$

## Inferência estatística

- $H_0: \beta_1 = \beta_2 = \beta_3$ pode ser expressa como $H_0: L\beta = 0$, pois se

\begin{eqnarray*}
L\beta &=& \left(\begin{array}{ccc}
1 & -1 & 0 \\
1 & 0 & -1
\end{array}\right) \left(\begin{array}{c}
\beta_1 \\
\beta_2 \\
\beta_3
\end{array}\right)\\
 &=& \left(\begin{array}{c}
\beta_1 - \beta_2 \\
\beta_1 - \beta_3
\end{array}\right) = 0,
\end{eqnarray*}

e então

$$
\left(\begin{array}{c}
\beta_1 \\
\beta_1
\end{array}\right) = \left(\begin{array}{c}
\beta_2 \\
\beta_3
\end{array}\right).
$$

## Inferência estatística

- Em geral, suponha que $L$ tem $r$ linhas (representando $r$ contrastes de interesse científico), então um teste simultâneo de $H_0: L\beta = 0$ versus $H_A: L\beta \neq 0$ é dado por

$$
W^2 = (L\hat{\beta})'\{L\widehat{\Cov}(\hat{\beta})L'\}^{-1}(L\hat{\beta}),
$$

que tem uma distribuição $\chi^2$ com $r$ gl.

- Este é o teste de Wald multivariado.

## Inferência estatística

- Uma alternativa para o teste de Wald é o __teste da razão de verossimilhanças__.
- O teste da razão de verossimilhanças de $H_0: L\beta = 0$ versus $H_A: L\beta \neq 0$ é obtido comparando as log-verossimilhanças maximizada para dois modelos:
    + um modelo incorpora a restrição que $L\beta = 0$ (por exemplo, $\beta_3 = 0$ ou $\beta_2 = \beta_3$); este será chamado de __modelo reduzido__;
    + e outro modelo sem restrição; este será chamado de __modelo completo__.

## Inferência estatística

- Uma estatística de teste é obtida por

$$
G^2 = 2(\hat{l}_{\mbox{comp}} - \hat{l}_{\mbox{red}}),
$$

e comparamos esta estatística com uma distribuição qui-quadrado com graus de liberdade igual a diferença entre o número de parâmetros nos modelos completo e reduzido.

- __Observação:__ o teste da razão de verossimilhança só pode ser usado quando o número de observações para os modelos completo e reduzido for o mesmo; __atenção com dados ausentes nas covariáveis__.

## Inferência estatística

- O uso da verossimilhança também pode fornecer limites de confiança para $\beta$ ou $L\beta$.
- Para um único componente de $\beta$, digamos $\beta_k$, podemos definir uma função __log-verossimilhança perfilada__, $l_p(\beta_k)$, obtida maximizando a log-verossimilhança sobre os parâmetros restantes, mantendo $\beta_k$ em algum valor fixo.

## Inferência estatística

- Um intervalo de confiança baseado na verossimilhança para $\beta_k$ é obtido considerando-se valores de $\beta_k$ que são razoavelmente consistentes com os dados.
    + Especificamente, um intervalo de confiança aproximado de 95% baseado na verossimilhança é dado pelo conjunto de todos os valores de $\beta_k$ que satisfazem
    
$$
2 \times \{l_p(\hat{\beta}_k) - l_p(\beta_k)\} \leq 3,84,
$$
em que o valor crítico $3,84$ é obtido da distribuição qui-quadrado com $1$ grau de liberdade.

- De maneira mais geral, os intervalos de confiança para $L\beta$ podem ser obtidos invertendo o teste correspondente de $H_0: L\beta = 0$ de maneira semelhante.

## Comentários

- Embora a construção de testes de razão de verossimilhança e intervalos de confiança com base em verossimilhança seja mais enredada (por exemplo, exigindo um ajuste adicional do modelo sob a hipótese nula) do que os correspondentes testes e intervalos de confiança com base na estatística de Wald, os testes e intervalos de confiança baseados em verossimilhança geralmente têm __propriedades superiores__.
- Este é especialmente o caso quando a variável de resposta é discreta.
    + Por exemplo, na regressão logística com dados binários, os testes de razão de verossimilhança têm melhores propriedades que os testes de Wald correspondentes.
- Portanto, em caso de dúvida, recomenda-se o uso de testes e intervalos de confiança baseados em verossimilhança. 

## Comentários

- Notamos que testes de razão de verossimilhança também podem ser usados para hipóteses sobre os __parâmetros de covariância__.
- No entanto, existem alguns problemas em potencial com o uso padrão do teste da razão de verossimilhança para comparar modelos aninhados para a covariância; retornaremos a este tópico quando discutirmos a modelagem da estrutura de covariância.
- Em geral, não é recomendado testar hipóteses sobre os parâmetros de covariância usando testes de Wald.
    + Em particular, a distribuição amostral da estatística do teste Wald para um parâmetro de variância não possui uma distribuição normal aproximada quando o tamanho da amostra é relativamente pequeno e a variância populacional é próxima de zero.
    + Como a variância tem um limite inferior igual a zero, são necessárias amostras muito grandes para justificar a aproximação normal para a distribuição amostral da estatística do teste de Wald quando a variância está próxima de zero.

# Exercícios

## Exercícios

1. (Re)fazer as provas dos resultados discutidos na aula de hoje.
2. Utilize os dados do estudo dos níveis de chumbo no sangue (TLC).
    + Proponha um modelo de regressão linear para a média com base nas questões de pesquisa.
    + Encontre as estimativas para os coeficientes de regressão do modelo proposto (use a função `gls` do pacote `nlme` do `R`; explore diferentes especificações da covariância - veja a documentação de `corClasses`).
    + __Com base nas estimativas__, faça a exposição das suas conclusões.

## Avisos

- __Próxima semana:__ lista de exercícios nº 1.
    + O momento da aula será utilizado para a resolução dos exercícios da lista.
- __Para casa:__ ler o Capítulo 4 do livro "__Applied Longitudinal Analysis__".
    + Fazer um resumo da Seção 4.5.
    + Caso ainda não tenha lido, leia também os Caps. 1, 2 e 3.
    + Ver também as referências com respeito à derivadas de vetores, matrizes, etc.
- __Próxima aula (1º/10):__ Modelando a média através da análise de perfis de respostas.

## Bons estudos!

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, out.width='70%', paged.print=FALSE, purl=FALSE}
knitr::include_graphics(here::here('images', 'estimation_icon.jpg'))
```
